name: 微调 Qwen-7B 高中学习助手（FastChat + LoRA）
on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  train-qwen-lora:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: write
    steps:
      - name: 拉取代码
        uses: actions/checkout@v4

      - name: 配置 Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      # 优先安装 PyTorch（适配 CUDA 11.8）
      - name: 优先安装 PyTorch（适配 CUDA 11.8）
        run: |
          pip install --upgrade pip
          pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118

      # 直接安装 FastChat 及其他依赖（跳过 flash-attn）
      - name: 安装 FastChat 及其他训练依赖
        run: |
          pip install -e .
          pip install transformers==4.37.2 accelerate==0.27.1 peft==0.8.2 datasets==2.14.6
          pip install huggingface-hub==0.20.3 sentencepiece==0.1.99

      - name: 安装 CUDA 11.8
        uses: Jimver/cuda-toolkit@v0.2.10
        with:
          cuda: '11.8'

      # 后续步骤（登录 HF、下载模型、训练）不变，仅训练命令添加 --no-flash-attn
      - name: 启动 Qwen 模型 LoRA 微调
        run: |
          python -m fastchat.train.train_lora \
            --model_name_or_path ./models/Qwen-7B \
            --data_path ./data/highschool_qa.json \
            --output_dir ./output/qwen-7b-highschool-lora \
            --per_device_train_batch_size=4 \
            --gradient_accumulation_steps=4 \
            --learning_rate=2e-4 \
            --num_train_epochs=3 \
            --logging_steps=10 \
            --save_steps=100 \
            --fp16=True \
            --lora_r=8 \
            --lora_alpha=16 \
            --lora_dropout=0.05 \
            --target_modules="q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj" \
            --peft_model_id ${{ secrets.HF_USERNAME }}/qwen-7b-highschool-qa \
            --push_to_hub=True \
            --no-flash-attn  # 关键：禁用 flash-attn
